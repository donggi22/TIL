{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1337337",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc8a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 입력 벡터 시퀀스 X\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "])\n",
    "\n",
    "# Q 가중치\n",
    "w_query = torch.tensor([\n",
    "  [1.0, 0.0, 1.0],\n",
    "  [1.0, 0.0, 0.0],\n",
    "  [0.0, 0.0, 1.0],\n",
    "  [0.0, 1.0, 1.0]\n",
    "])\n",
    "\n",
    "# K 가중치\n",
    "w_key = torch.tensor([\n",
    "  [0.0, 0.0, 1.0],\n",
    "  [1.0, 1.0, 0.0],\n",
    "  [0.0, 1.0, 0.0],\n",
    "  [1.0, 1.0, 0.0]\n",
    "])\n",
    "\n",
    "# V 가중치\n",
    "w_value = torch.tensor([\n",
    "  [0.0, 2.0, 0.0],\n",
    "  [0.0, 3.0, 0.0],\n",
    "  [1.0, 0.0, 3.0],\n",
    "  [1.0, 1.0, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9ac98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 1., 3.]]),\n",
       " tensor([[0., 1., 1.],\n",
       "         [4., 4., 0.],\n",
       "         [2., 3., 1.]]),\n",
       " tensor([[1., 2., 3.],\n",
       "         [2., 8., 0.],\n",
       "         [2., 6., 3.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 모델에서는 입력 벡터 시퀀스 Pre-LayerNorm 적용\n",
    "# x = layer_norm(x)\n",
    "# 초기 논문은 Post-LayerNorm 구조였음 Attention(Q,K,V) + X → LayerNorm\n",
    "\n",
    "querys = torch.mm(x, w_query)\n",
    "keys = torch.mm(x, w_key)\n",
    "values = torch.mm(x, w_value)\n",
    "\n",
    "querys, keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f919b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  4.],\n",
       "        [ 4., 16., 12.],\n",
       "        [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.matmul(querys, keys.T)\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21aef478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7321)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "key_dim_sqrt = torch.sqrt(torch.tensor(keys.shape[-1]))\n",
    "key_dim_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88afae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n",
       "        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n",
       "        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_softmax = softmax(attn_scores / key_dim_sqrt, dim=-1)\n",
    "attn_scores_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e58a6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8639, 6.3194, 1.7042],\n",
       "        [1.9991, 7.8141, 0.2735],\n",
       "        [1.9926, 7.4796, 0.7359]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_values = torch.matmul(attn_scores_softmax, values)\n",
    "weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd714b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "# 입력 (T=3(시퀀스 길이), d_model=4(임베딩 차원))\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "])\n",
    "\n",
    "# 멀티헤드 설정\n",
    "T, d_model = x.shape\n",
    "num_heads = 2\n",
    "head_dim = d_model // num_heads   # 2\n",
    "assert d_model % num_heads == 0\n",
    "\n",
    "# (예제용) Q/K/V 가중치: (d_model, d_model)\n",
    "# 실제 구현에서는 nn.Linear(d_model, d_model) 3개로 만드는 경우가 많습니다.\n",
    "# self.q_proj = nn.Linear(d_model, d_model)\n",
    "# self.k_proj = nn.Linear(d_model, d_model)\n",
    "# self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "# head별 Q, K, V projection을 하나로 합친 행렬 (2 heads × 2-dim(각 head의 임베딩 차원) = d_mode(토큰 임베딩 차원))\n",
    "W_Q = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "W_K = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "W_V = torch.tensor([\n",
    "    [1.0, 0.0, 2.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 2.0],\n",
    "    [1.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0, 0.0]\n",
    "])\n",
    "\n",
    "# 출력 투영 W_O: (d_model, d_model)\n",
    "# 예제에서는 단순히 섞임이 보이도록 임의 값 사용(혹은 torch.eye(4)로 identity도 가능)\n",
    "W_O = torch.tensor([\n",
    "    [1.0, 0.0, 0.5, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.5],\n",
    "    [0.5, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.5, 0.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d884acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 2., 0.],\n",
       "         [2., 2., 0., 4.],\n",
       "         [2., 2., 2., 2.]]),\n",
       " tensor([[0., 2., 1., 1.],\n",
       "         [4., 0., 2., 2.],\n",
       "         [2., 2., 2., 2.]]),\n",
       " tensor([[2., 0., 2., 1.],\n",
       "         [0., 4., 2., 4.],\n",
       "         [2., 2., 3., 3.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Q, K, V 만들기: (T, d_model)\n",
    "Q = x @ W_Q\n",
    "K = x @ W_K\n",
    "V = x @ W_V\n",
    "\n",
    "Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63dfdf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qh: tensor([[[1., 1.],\n",
      "         [2., 0.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [0., 4.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.]]]), \n",
      "\n",
      " Kh: tensor([[[0., 2.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[4., 0.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.]]]), \n",
      "\n",
      " Vh: tensor([[[2., 0.],\n",
      "         [2., 1.]],\n",
      "\n",
      "        [[0., 4.],\n",
      "         [2., 4.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "# 2) 헤드로 reshape: (T, num_heads, head_dim)\n",
    "Qh = Q.view(T, num_heads, head_dim)\n",
    "Kh = K.view(T, num_heads, head_dim)\n",
    "Vh = V.view(T, num_heads, head_dim)\n",
    "\n",
    "print(f'Qh: {Qh}, \\n\\n Kh: {Kh}, \\n\\n Vh: {Vh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a3552d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 0.],\n",
       "         [0., 4.],\n",
       "         [2., 2.]]),\n",
       " tensor([[1., 2., 2.],\n",
       "         [1., 2., 2.]]),\n",
       " tensor([[2., 4., 4.],\n",
       "         [4., 8., 8.],\n",
       "         [4., 8., 8.]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qh[:, 1, :], Kh[:, 1, :].T, Qh[:, 1, :] @ Kh[:, 1, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8ca1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 4., 4.],\n",
       "         [4., 8., 8.],\n",
       "         [4., 8., 8.]],\n",
       "\n",
       "        [[2., 4., 4.],\n",
       "         [4., 8., 8.],\n",
       "         [4., 8., 8.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) 헤드별 어텐션 스코어: (num_heads, T, T)\n",
    "\n",
    "# head마다 Qh[:,h,:] @ Kh[:,h,:].T\n",
    "# scores = torch.zeros(num_heads, T, T)\n",
    "# for h in range(num_heads):\n",
    "#     scores[h] = Qh[:, h, :] @ Kh[:, h, :].T # (3, 2) @ (2, 3)\n",
    "\n",
    "# 내부적으로는 위 코드가 작동됨.\n",
    "scores = torch.einsum(\"thd,shd->hts\", Qh, Kh)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66c972d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1084, 0.4458, 0.4458],\n",
       "         [0.0287, 0.4856, 0.4856],\n",
       "         [0.0287, 0.4856, 0.4856]],\n",
       "\n",
       "        [[0.1084, 0.4458, 0.4458],\n",
       "         [0.0287, 0.4856, 0.4856],\n",
       "         [0.0287, 0.4856, 0.4856]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스케일링: / sqrt(head_dim)\n",
    "scale = torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "attn = softmax(scores / scale, dim=-1)   # (h, T, T)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e95f6a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1084, 2.6748],\n",
       "         [2.4458, 3.2290]],\n",
       "\n",
       "        [[1.0287, 2.9139],\n",
       "         [2.4856, 3.4282]],\n",
       "\n",
       "        [[1.0287, 2.9139],\n",
       "         [2.4856, 3.4282]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) 헤드별 weighted sum: (num_heads, T, head_dim)\n",
    "out_heads = torch.einsum(\"hts,shd->thd\", attn, Vh)  # (T, h, d_head)\n",
    "out_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "210ee815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1084, 2.6748, 2.4458, 3.2290],\n",
       "        [1.0287, 2.9139, 2.4856, 3.4282],\n",
       "        [1.0287, 2.9139, 2.4856, 3.4282]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) concat: (T, d_model)\n",
    "out_concat = out_heads.reshape(T, d_model)\n",
    "out_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2880c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3313, 4.2894, 3.0000, 4.5665],\n",
       "        [2.2715, 4.6280, 3.0000, 4.8852],\n",
       "        [2.2715, 4.6280, 3.0000, 4.8852]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) 출력 투영: (T, d_model)\n",
    "out = out_concat @ W_O\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c308da6",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Masked Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4d0921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "# 입력 (T=3, d_model=4)\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "])\n",
    "\n",
    "# 멀티헤드 설정\n",
    "T, d_model = x.shape\n",
    "num_heads = 2\n",
    "head_dim = d_model // num_heads\n",
    "assert d_model % num_heads == 0\n",
    "\n",
    "# (예제용) Q/K/V 가중치: (d_model, d_model)\n",
    "W_Q = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "W_K = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "W_V = torch.tensor([\n",
    "    [1.0, 0.0, 2.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 2.0],\n",
    "    [1.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0, 0.0]\n",
    "])\n",
    "\n",
    "# 출력 투영 W_O: (d_model, d_model)\n",
    "W_O = torch.tensor([\n",
    "    [1.0, 0.0, 0.5, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.5],\n",
    "    [0.5, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.5, 0.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "726d4234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 2., 0.],\n",
       "         [2., 2., 0., 4.],\n",
       "         [2., 2., 2., 2.]]),\n",
       " tensor([[0., 2., 1., 1.],\n",
       "         [4., 0., 2., 2.],\n",
       "         [2., 2., 2., 2.]]),\n",
       " tensor([[2., 0., 2., 1.],\n",
       "         [0., 4., 2., 4.],\n",
       "         [2., 2., 3., 3.]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Q, K, V 만들기: (T, d_model)\n",
    "Q = x @ W_Q\n",
    "K = x @ W_K\n",
    "V = x @ W_V\n",
    "\n",
    "Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1601247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qh: tensor([[[1., 1.],\n",
      "         [2., 0.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [0., 4.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.]]]), \n",
      "\n",
      " Kh: tensor([[[0., 2.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[4., 0.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.]]]), \n",
      "\n",
      " Vh: tensor([[[2., 0.],\n",
      "         [2., 1.]],\n",
      "\n",
      "        [[0., 4.],\n",
      "         [2., 4.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "# 2) 헤드로 reshape: (T, h, d_head)\n",
    "Qh = Q.view(T, num_heads, head_dim)\n",
    "Kh = K.view(T, num_heads, head_dim)\n",
    "Vh = V.view(T, num_heads, head_dim)\n",
    "\n",
    "print(f'Qh: {Qh}, \\n\\n Kh: {Kh}, \\n\\n Vh: {Vh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b133032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 4., 4.],\n",
       "         [4., 8., 8.],\n",
       "         [4., 8., 8.]],\n",
       "\n",
       "        [[2., 4., 4.],\n",
       "         [4., 8., 8.],\n",
       "         [4., 8., 8.]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) 헤드별 어텐션 점수: (h, T, T)\n",
    "scores = torch.einsum(\"thd,shd->hts\", Qh, Kh)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6e58300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Causal mask 만들기\n",
    "# mask[t, s] = True면 \"가려야 함\" (s가 미래: s > t)\n",
    "causal_mask = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)  # (T, T)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee3848fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., -inf, -inf],\n",
       "         [4., 8., -inf],\n",
       "         [4., 8., 8.]],\n",
       "\n",
       "        [[2., -inf, -inf],\n",
       "         [4., 8., -inf],\n",
       "         [4., 8., 8.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores에 적용하려면 (h, T, T)로 broadcast되게 (1, T, T)로 확장\n",
    "scores_masked = scores.masked_fill(causal_mask.unsqueeze(0), float(\"-inf\"))\n",
    "scores_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb8986fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000],\n",
       "         [0.0558, 0.9442, 0.0000],\n",
       "         [0.0287, 0.4856, 0.4856]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000],\n",
       "         [0.0558, 0.9442, 0.0000],\n",
       "         [0.0287, 0.4856, 0.4856]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) softmax + weighted sum\n",
    "scale = torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "attn = softmax(scores_masked / scale, dim=-1)  # (h, T, T)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5176cfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0000, 0.0000],\n",
       "         [2.0000, 1.0000]],\n",
       "\n",
       "        [[0.1116, 3.7768],\n",
       "         [2.0000, 3.8326]],\n",
       "\n",
       "        [[1.0287, 2.9139],\n",
       "         [2.4856, 3.4282]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_heads = torch.einsum(\"hts,shd->thd\", attn, Vh)  # (T, h, d_head)\n",
    "out_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97c58cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 2.0000, 1.0000],\n",
       "        [0.1116, 3.7768, 2.0000, 3.8326],\n",
       "        [1.0287, 2.9139, 2.4856, 3.4282]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) concat + output projection\n",
    "out_concat = out_heads.reshape(T, d_model)\n",
    "out_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07b442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000, 0.5000, 3.0000, 1.0000],\n",
       "        [1.1116, 5.6931, 2.0558, 5.7210],\n",
       "        [2.2715, 4.6280, 3.0000, 4.8852]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out_concat @ W_O\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58760d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "| 정규화                    | 평균·분산을 내는 축              |\n",
    "| ---------------------- | ------------------------ |\n",
    "| **LayerNorm (층 정규화)**  | **feature 축** (`dim=-1`) |\n",
    "| **BatchNorm (배치 정규화)** | **batch 축** (`dim=0`)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98107cf",
   "metadata": {},
   "source": [
    "# LayerNorm\n",
    "\n",
    "입력 벡터 $(x \\in \\mathbb{R}^{d}$) (한 샘플의 feature 벡터)에 대해\n",
    "\n",
    "### 평균\n",
    "$$\n",
    "\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i\n",
    "$$\n",
    "\n",
    "### 분산\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "### 정규화\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "### Affine 변환\n",
    "$$\n",
    "y_i = \\gamma_i \\hat{x}_i + \\beta_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cff2d7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247,  0.0000,  1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "m = torch.nn.LayerNorm(input.shape[-1])\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b578639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight, m.bias # weight는 γ, bias는 β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd36e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 1.]), tensor([0.8165, 0.0000]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(input, dim=-1), torch.std(input, dim=-1, unbiased=False) \n",
    "# unbiased=True는 표본분산(N−1)이고, LayerNorm/BatchNorm에서는 unbiased=False(모분산)를 쓴다.\n",
    "# 지금 들어온 텐서를 그대로 정규화, “모집단 추정” 개념이 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e8db786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247,  0.0000,  1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input - torch.mean(input, -1).view(-1, 1))  / (torch.std(input,-1, unbiased=False)+ 1e-7).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431cde7",
   "metadata": {},
   "source": [
    "# BatchNorm\n",
    "\n",
    "입력 행렬 ($x \\in \\mathbb{R}^{N \\times C}$)  \n",
    "($N$: batch size, $C$: feature/channel)\n",
    "\n",
    "### 평균 (채널별, 배치 기준)\n",
    "$$\n",
    "\\mu_c = \\frac{1}{N}\\sum_{n=1}^{N} x_{n,c}\n",
    "$$\n",
    "\n",
    "### 분산 (채널별, 배치 기준)\n",
    "$$\n",
    "\\sigma_c^2 = \\frac{1}{N}\\sum_{n=1}^{N} (x_{n,c} - \\mu_c)^2\n",
    "$$\n",
    "\n",
    "### 정규화\n",
    "$$\n",
    "\\hat{x}_{n,c} = \\frac{x_{n,c} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "### Affine 변환\n",
    "$$\n",
    "y_{n,c} = \\gamma_c \\hat{x}_{n,c} + \\beta_c\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47f370e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  1.0000],\n",
       "        [ 0.0000, -1.0000, -1.0000]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "bn = torch.nn.BatchNorm1d(input.shape[-1])\n",
    "output = bn(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aea7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.weight, bn.bias # weight는 γ, bias는 β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa2ae631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 1.5000, 2.0000]), tensor([0.0000, 0.5000, 1.0000]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(input, dim=0), torch.std(input, dim=0, unbiased=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1e71cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  1.0000],\n",
       "        [ 0.0000, -1.0000, -1.0000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input - torch.mean(input, 0).view(1, -1))  / (torch.std(input, 0, unbiased=False)+ 1e-7).view(1, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
