{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea3e08e",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baa58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 작업 준비\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18688577",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf6e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = datasets.FashionMNIST(root='../data/',\n",
    "                              train=True,\n",
    "                              download=False,\n",
    "                              transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5012a407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 60000\n",
    "tr_ds_loader = torch.utils.data.DataLoader(\n",
    "    dataset = tr_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "img, _ = next(iter(tr_ds_loader))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8115aab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2860), tensor(0.3530))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.mean(), img.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68c05fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7908f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Tensor화\n",
    "    transforms.Normalize(0.2860, 0.3530) # torch는 Normalize 전에 Tensor화 돼야 함!\n",
    "])\n",
    "\n",
    "tr_ds_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        root='../data/',\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tt_ds_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        root='../data/',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cf2f6",
   "metadata": {},
   "source": [
    "$$O=\\frac{I-F+2P}{S}+1$$\n",
    "\n",
    "```py\n",
    "28 * 28 * 1 # 입력 사이즈\n",
    "-> 24 * 24 * 10\n",
    "-> 12 * 12 * 10\n",
    "    -> 8 * 8 * 20\n",
    "    -> 4 * 4 * 20 = 320\n",
    "\n",
    "    # 또는 padding=same 일 때\n",
    "    -> 12 * 12 * 20\n",
    "    -> 6 * 6 * 20 = 720\n",
    "\n",
    "# --------------------------\n",
    "\n",
    "# 또는 padding=same 일 때\n",
    "28 * 28 * 1\n",
    "-> 28 * 28 * 10  \n",
    "-> 14 * 14 * 10  \n",
    "    -> 10 * 10 * 20  \n",
    "    -> 5 * 5 * 20 = 500\n",
    "\n",
    "    # 또는 padding=same 일 때\n",
    "    -> 14 * 14 * 20  \n",
    "    -> 7 * 7 * 20 = 980\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24298fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2)\n",
    "        x = F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0043cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "# 학습에서 스케줄 계획하는 건 학습률 외엔 거의 없음.\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # 가급적 에폭마다 스케줄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaca071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tr_ds_loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for (x, y) in tr_ds_loader:\n",
    "        data, target = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += len(data)\n",
    "        running_loss += loss.item() * len(data)\n",
    "        train_loss = running_loss / total\n",
    "        correct += (output.argmax(dim=1) == target).sum().item()\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        tr_ds_loader.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'train_accuracy': f'{train_accuracy * 100:.2f}%'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "814fed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, tt_ds_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    for data, target in tt_ds_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).item() * len(data)\n",
    "        correct += (output.argmax(dim=1) == target).sum().item()\n",
    "    test_loss /= len(tt_ds_loader.dataset)\n",
    "    test_accuracy = correct / len(tt_ds_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83f7e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 1]: 100%|██████████| 938/938 [00:16<00:00, 56.57it/s, train_loss=0.4908, train_accuracy=82.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.3798, test_accuracy: 86.09%, lr_scheduler[0.001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 2]: 100%|██████████| 938/938 [00:17<00:00, 52.30it/s, train_loss=0.4887, train_accuracy=82.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.3796, test_accuracy: 86.20%, lr_scheduler[0.001]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 3]: 100%|██████████| 938/938 [00:16<00:00, 57.31it/s, train_loss=0.4857, train_accuracy=82.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.3768, test_accuracy: 86.23%, lr_scheduler[0.0001]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "EPOCHS = 3\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_bar = tqdm(\n",
    "        tr_ds_loader,\n",
    "        desc=f'[Epochs: {epoch}]',\n",
    "        leave=True\n",
    "        )\n",
    "    train(model, train_bar, optimizer)\n",
    "    scheduler.step()\n",
    "    test_loss, test_accuracy = evaluate(model, tt_ds_loader)\n",
    "\n",
    "    tqdm.write(\n",
    "        f'test_loss: {test_loss:.4f}, '\n",
    "        f'test_accuracy: {test_accuracy * 100:.2f}%, '\n",
    "        f'lr_scheduler{scheduler.get_last_lr()}'\n",
    "        '\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1713363",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_tr = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "tr_ds = datasets.CIFAR10(root='../data/',\n",
    "                 train=True,\n",
    "                 download=True,\n",
    "                 transform=ck_tr)\n",
    "\n",
    "tr_ds_loader = torch.utils.data.DataLoader(tr_ds,\n",
    "                                           batch_size=50000,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "613f4fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 32, 32])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck_data = iter(tr_ds_loader)\n",
    "data, _ = next(ck_data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6802899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4914) tensor(0.2470)\n",
      "tensor(0.4822) tensor(0.2435)\n",
      "tensor(0.4465) tensor(0.2616)\n"
     ]
    }
   ],
   "source": [
    "print(data[:, 0].mean(), data[:, 0].std())\n",
    "print(data[:, 1].mean(), data[:, 1].std())\n",
    "print(data[:, 2].mean(), data[:, 2].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2858f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4914, 0.4822, 0.4465]), tensor([0.2470, 0.2435, 0.2616]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 코드를 한 번에!\n",
    "data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3]) # 0,2,3번 차원 전체를 뭉텅이로 축소하고 1번 차원(channel)만 남겨 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eef5b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상수 설정\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4f94fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수정 (노이즈 삽입)\n",
    "# 1. 데이터 준비\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "tr_ds_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root='../data/',\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tt_ds_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root='../data/',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd7598",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 설계\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False), \n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, class_n):\n",
    "        super().__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.l1 = self._make_l(16, 2, 1)\n",
    "        self.l2 = self._make_l(32, 2, 1)\n",
    "        self.l3 = self._make_l(64, 2, 1)\n",
    "        self.out_l = nn.Linear(1024, class_n)\n",
    "\n",
    "    def _make_l(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        l = []\n",
    "        for stride in strides:\n",
    "            l.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*l)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "\n",
    "        # 분류기 층\n",
    "        x = F.avg_pool2d(x, 8)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.out_l(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "faf91334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(10).to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db56153d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (l2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (l3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (out_l): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c003b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tr_ds_loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for (x, y) in tr_ds_loader:\n",
    "        data, target = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += len(data)\n",
    "        running_loss += loss.item() * len(data)\n",
    "        pred = (output.argmax(dim=1) == target)\n",
    "        correct += pred.sum().item()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        tr_ds_loader.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'train_accuracy': f'{train_accuracy*100:.2f}%'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07b92faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, tt_ds_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    for x, y in tt_ds_loader:\n",
    "        data, target = x.to(DEVICE), y.to(DEVICE)\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target).item()\n",
    "        test_loss += loss * len(data)\n",
    "        pred = (output.argmax(dim=1) == target)\n",
    "        correct += pred.sum().item()\n",
    "    test_loss /= len(tt_ds_loader.dataset)\n",
    "    test_accuracy = correct / len(tt_ds_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 1/20]: 100%|██████████| 782/782 [00:44<00:00, 17.51it/s, train_loss=2.1054, train_accuracy=24.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.8224, test_accuracy: 35.56%, lr: [0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 2/20]: 100%|██████████| 782/782 [00:42<00:00, 18.47it/s, train_loss=1.6438, train_accuracy=40.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.6426, test_accuracy: 41.25%, lr: [0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 3/20]: 100%|██████████| 782/782 [00:42<00:00, 18.49it/s, train_loss=1.4402, train_accuracy=48.71%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.4143, test_accuracy: 47.68%, lr: [0.010000000000000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 4/20]: 100%|██████████| 782/782 [00:42<00:00, 18.58it/s, train_loss=1.0878, train_accuracy=61.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.0760, test_accuracy: 61.89%, lr: [0.010000000000000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 5/20]: 100%|██████████| 782/782 [00:42<00:00, 18.41it/s, train_loss=1.0094, train_accuracy=64.31%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.9864, test_accuracy: 65.11%, lr: [0.010000000000000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 6/20]: 100%|██████████| 782/782 [00:42<00:00, 18.39it/s, train_loss=0.9514, train_accuracy=66.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.9516, test_accuracy: 66.73%, lr: [0.0010000000000000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 7/20]: 100%|██████████| 782/782 [00:42<00:00, 18.37it/s, train_loss=0.8668, train_accuracy=69.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8774, test_accuracy: 69.00%, lr: [0.0010000000000000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 8/20]: 100%|██████████| 782/782 [00:42<00:00, 18.33it/s, train_loss=0.8508, train_accuracy=70.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8675, test_accuracy: 69.33%, lr: [0.0010000000000000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 9/20]: 100%|██████████| 782/782 [00:42<00:00, 18.58it/s, train_loss=0.8394, train_accuracy=70.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8608, test_accuracy: 69.80%, lr: [0.00010000000000000003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 10/20]: 100%|██████████| 782/782 [00:42<00:00, 18.59it/s, train_loss=0.8258, train_accuracy=71.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8525, test_accuracy: 69.89%, lr: [0.00010000000000000003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 11/20]: 100%|██████████| 782/782 [00:42<00:00, 18.61it/s, train_loss=0.8221, train_accuracy=71.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8513, test_accuracy: 69.90%, lr: [0.00010000000000000003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 12/20]: 100%|██████████| 782/782 [00:42<00:00, 18.34it/s, train_loss=0.8208, train_accuracy=71.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8509, test_accuracy: 70.02%, lr: [1.0000000000000004e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 13/20]: 100%|██████████| 782/782 [00:42<00:00, 18.47it/s, train_loss=0.8209, train_accuracy=71.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8507, test_accuracy: 69.95%, lr: [1.0000000000000004e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 14/20]: 100%|██████████| 782/782 [00:42<00:00, 18.34it/s, train_loss=0.8200, train_accuracy=71.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8513, test_accuracy: 69.94%, lr: [1.0000000000000004e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 15/20]: 100%|██████████| 782/782 [00:42<00:00, 18.26it/s, train_loss=0.8191, train_accuracy=71.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8509, test_accuracy: 69.99%, lr: [1.0000000000000004e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs: 16/20]:  95%|█████████▌| 744/782 [00:40<00:02, 18.72it/s, train_loss=0.8198, train_accuracy=71.48%]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_bar = tqdm(\n",
    "        tr_ds_loader,\n",
    "        desc=f'[Epochs: {epoch}/{EPOCHS}]',\n",
    "        leave=True\n",
    "    )\n",
    "    train(model, train_bar, optimizer)\n",
    "    scheduler.step()\n",
    "    test_loss, test_accuracy = evaluate(model, tt_ds_loader)\n",
    "    tqdm.write(\n",
    "        f'test_loss: {test_loss:.4f}, '\n",
    "        f'test_accuracy: {test_accuracy * 100:.2f}%, '\n",
    "        f'lr: {scheduler.get_last_lr()}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1220f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시스템 전체 논리 코어 수: 8\n",
      "시스템 전체 물리 코어 수: 4\n",
      "PyTorch로 설정한 스레드 수: 100\n"
     ]
    }
   ],
   "source": [
    "import torch, os, psutil\n",
    "\n",
    "print(\"시스템 전체 논리 코어 수:\", os.cpu_count())\n",
    "print(\"시스템 전체 물리 코어 수:\", psutil.cpu_count(logical=False))\n",
    "print(\"PyTorch로 설정한 스레드 수:\", torch.get_num_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7180b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch로 설정한 스레드 수: 12\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(12)\n",
    "print(\"PyTorch로 설정한 스레드 수:\", torch.get_num_threads()) # 실제 8개가 최대라서 12개로 하면 오버헤드 걸림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d06ed949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch로 설정한 스레드 수: 100\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(100)\n",
    "print(\"PyTorch로 설정한 스레드 수:\", torch.get_num_threads()) # 실제 8개가 최대라서 100개로 하면 오버헤드 걸림."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
